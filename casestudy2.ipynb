{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN645 Case Study 2\n",
    "## Mining from Manufacturing, Supermarket, News Stories and Web Log Data\n",
    "\n",
    "### Contents\n",
    "1. [Clustering & Pre-processing](#clust)\n",
    "2. [Association Mining](#association)\n",
    "3. [Text Mining](#text)\n",
    "4. [Web Mining](#web)\n",
    "\n",
    "---\n",
    "## Part 1: Clustering Pre-processing and K-means analysis<a name=\"clust\"></a>\n",
    "### 1. Can you identify data quality issues in this dataset such as unusual data types, missing values, etc?\n",
    "In the process of importing the data, the dataframe.info() method can be used to evaluate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Import Data from csv without skipping empty cells\n",
    "df = pd.read_csv('Casestudy2-Data-Py/model_car_sales.csv', na_filter=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `info()` output, we can identify the following issues\n",
    "\n",
    "#### Unusual Datatypes\n",
    "<table>\n",
    "<tr>\n",
    "<th>Variable Name</th>\n",
    "<th>Current Datatype</th>\n",
    "<th>Desired Datatype</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HATCH</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEDAN</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WAGON</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>UTE</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>K_SALES_TOT </td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "</table>\n",
    "     \n",
    "For the given data description, the fields `UTE`, `HATCH`,`SEDAN`, `WAG0N` and `K_SALES_TOT` should be interval/numerical values as opposed to objects.\n",
    "By using the `.describe()` function, we may be able to uncover the source of the issues in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print details for all variables in dataframe\n",
    "for cols in df:\n",
    "    print(df[cols].describe())\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the `.describe()` output show a number of unusual results which may explain the cause of the unusual data types.\n",
    "\n",
    "For this output, `top` describes the most commonly occuring value in the dataset. For the fields `UTE`, `HATCH`, `WAG0N` and `SEDAN`, that value appears to be an empty string.\n",
    "\n",
    "The output of the `value_counts()` function (see below) allow us to identify this as the root cause of our data abnormalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for cause of issues in one of the variables\n",
    "print(df['UTE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can isolate the rows containing the empty string in order to determine if there is a link between the missing variables in the data source.\n",
    "By using the `.values()` or `as_matrix()` functions to gather all of the readings where UTE is equal to an empty string we ge the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Rows where UTE contains empty string\n",
    "print(df[df['UTE']==''].as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "This coencides with the missing values found present in the `HATCH`,`SEDAN` and `WAG0N` fields as well, additionally identifying 22 rows where the `K_SALES_TOT` variable is also missing. A tabulated view of the results can be found below.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable Name</th>\n",
    "        <th># Missing Values</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>HATCH</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SEDAN</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>WAGON</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>UTE</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>K_SALES_TOT</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "For our analysis of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Association Mining and it's data Pre-processing<a name=\"association\"></a>\n",
    "\n",
    "### 1. Can you identify data quality issues in this dataset for performing association mining?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the transaction dataset\n",
    "df = pd.read_csv('Casestudy2-Data-Py/pos_transactions.csv')\n",
    "\n",
    "# info and the first 10 transactions\n",
    "print(df.info())\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first 10 transaction shown above, we can easily see that there are duplicated transactions in the dataset.\n",
    "That can be seen clearly by grouping the data (see the result below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(df.columns.tolist(),as_index=False).size().sort_values(ascending = False).reset_index().rename(columns={0: 'Frequency'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Frequency' column in the result above indicates the number of duplicated transactions for each unique transaction in the dataset. Therefore, we will drop those duplicated transactions except for the first occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As were are looking to generate association rules from items purchased by each transaction, we need to group our Transaction_Id and then generate a list of all items purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group by Transaction_Id, then list all items\n",
    "transactions = df.groupby(['Transaction_Id'])['Product_Name'].apply(list)\n",
    "\n",
    "print(transactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "Association mining usually consists of two variables: a transaction ID and an item. Due to the main target which is to find out the associations between items purchased from the health and beauty aids department and the stationary department, the Product_Name should be chosen as the target variable and the Transaction_ID is chosen as ID. In summary, the variables included in the analysis as the below table.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable Name</th>\n",
    "        <th>Role</th>\n",
    "        <th>Measurement Level</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Product_Name</td>\n",
    "        <td>Target</td>\n",
    "        <td>Nominal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Transaction_Id</td>\n",
    "        <td>ID</td>\n",
    "        <td>Nominal</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conduct association mining and answer the following:\n",
    "#### a. What is the highest lift value for the resulting rules? Which rule has this value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the transactions table contains all items purchased in each transaction, we will run the apyori model with the pre-processed transactions and min_support of 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# type cast the transactions from pandas into normal list format and run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=0.02))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift']) \n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out the highest lift value for the resulting rules, we will sort the rules by Lift using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort all acquired rules descending by lift# sort a \n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the highest lift value is 3.60\n",
    "There are two rules having this value: Perfume -> Toothbrush and Toothbrush -> Perfume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is the highest confidence value for the resulting rules? Which rule has this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In order to find out the highest confidence value for the resulting rules, we will sort the rules by Confidence using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort all acquired rules descending by Confidence# sort a \n",
    "result_df = result_df.sort_values(by='Confidence', ascending=False)\n",
    "print(result_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the highest confidence value is 32.39%.\n",
    "The rule has this value is Magazie & Greeding Cards -> Candy Bar  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Plot the confidence, lift, support of the resulting rules? Interpret them to discuss the rule-set obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The store is particularly interested in products that individuals purchase when they buy “Pens”.\n",
    "#### a. How many rules are in the subset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Based on the rules, what are the other products these individuals are most likely to purchase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How the outcome of this study can be used by decision makers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Web Mining<a name=\"web\"></a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Text Mining<a name=\"text\"></a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected column is the TEXT column, as this column contains the text data from which information will be extracted. All other columns are rejected.\n",
    "\n",
    "The LANGUANGE, OMITTED, TRUNCATED, and EXTENSION columns are unary, and provide no meaningful data. Similarly, the URI, NAME, and FILTERED columns are incremental values that also provide no meaningful data. Finally, the columns CREATED, ACCESSED, and MODIFIED, appear to be date based values. Modified is binary, and the other columns only have thirty five unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED - Unique: 35\n",
      "ACCESSED - Unique: 35\n",
      "MODIFIED - Unique: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf = pd.read_csv(\"Casestudy2-Data-Py/bbc.csv\")\n",
    "\n",
    "textCols = [\"CREATED\", \"ACCESSED\", \"MODIFIED\"]\n",
    "\n",
    "for col in textCols:\n",
    "    print(col, \"- Unique:\", len(tf[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Can you identify data quality issues in order to perform text mining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# initialise WordNet lemmatizer and punctuation filter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "stopwords = set(sw.words('english'))\n",
    "textColumn = 'TEXT'\n",
    "rs = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 36360\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # initialize token list\n",
    "    tokens = []\n",
    "    \n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # split the document into tokens and then create part of speech tag for each token\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the tokens list\n",
    "            lemma = lemmatize(token, tag)\n",
    "            tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "print(\"Unique tokens:\", len(tfidf_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# K means clustering using the term vector\n",
    "kmeans = KMeans(n_clusters=7, random_state=rs).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
