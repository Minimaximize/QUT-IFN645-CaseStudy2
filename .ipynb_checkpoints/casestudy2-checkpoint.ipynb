{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unusual Datatypes\n",
    "<table>\n",
    "<tr>\n",
    "<th>Variable Name</th>\n",
    "<th>Current Datatype</th>\n",
    "<th>Desired Datatype</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HATCH</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEDAN</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WAGON</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>UTE</td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>K_SALES_TOT </td>\n",
    "<td>Object</td>\n",
    "<td>int64</td>\n",
    "</tr>\n",
    "</table>\n",
    "     \n",
    "For the given data description, the fields `UTE`, `HATCH`,`SEDAN`, `WAG0N` and `K_SALES_TOT` should be interval/numerical values as opposed to objects.\n",
    "By using the `.describe()` function, we may be able to uncover the source of the issues in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN645 Case Study 2\n",
    "## Mining from Manufacturing, Supermarket, News Stories and Web Log Data\n",
    "\n",
    "### Contents\n",
    "1. [Clustering & Pre-processing](#clust)\n",
    "2. [Association Mining](#association)\n",
    "3. [Text Mining](#text)\n",
    "4. [Web Mining](#web)\n",
    "\n",
    "---\n",
    "## Part 1: Clustering Pre-processing and K-means analysis<a name=\"clust\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Required for this section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Import Data for this section from csv (without skipping empty cells)\n",
    "df = pd.read_csv('Casestudy2-Data-Py/model_car_sales.csv', na_filter=False)\n",
    "\n",
    "# Get details about the dataset \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description of Dataset\n",
    "Below is a description of the data as it appears in the brief\n",
    "<table>\n",
    "<tr>\n",
    "<th>Variable</th>\n",
    "<th>Measurement Level</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LOCATION_NUMBER</td>\n",
    "<td>Nominal</td>\n",
    "<td>Numeric code for the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>REPORT_DATE</td>\n",
    "<td>Unary</td>\n",
    "<td>Date of the data extraction</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DEALER_CODE</td>\n",
    "<td>Nominal</td>\n",
    "<td>Text identifier for the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>UTE</td>\n",
    "<td>Interval</td>\n",
    "<td>Number of Utility/tray back model cars sold by the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HATCH</td>\n",
    "<td>Interval</td>\n",
    "<td>Number of Utility/tray back model cars sold by the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WAG0N</td>\n",
    "<td>Interval</td>\n",
    "<td>Number of Station Wagon model cars sold by the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEDAN</td>\n",
    "<td>Interval</td>\n",
    "<td>Number of Sedan model cars sold by the store</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>K__SALES_TOT</td>\n",
    "<td>Interval</td>\n",
    "<td>Total sales for the store</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Can you identify data quality issues in this dataset such as unusual data types, missing values, etc?\n",
    "By comparing the output from the `info()` method to the table above, a number of data quality issues emerge.\n",
    "\n",
    "The rows `HATCH`,`SEDAN`,`WAG0N`,`UTE` and `K__SALES_TOT`are of the type `object` in the dataframe, where the data description suggests that the values should be of the type `int` or `float`.\n",
    "\n",
    "Using the `.describe()` function may uncover the root of the problem within the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print details for all variables in dataframe\n",
    "def describeDataset(doc):\n",
    "    \"\"\"\n",
    "    Print Details for each column\n",
    "    \"\"\"\n",
    "    for cols in doc:\n",
    "        print(df[cols].describe())\n",
    "        print(\"-\"*20)\n",
    "        \n",
    "describeDataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the `.describe()` output show a number of unusual results which may explain the cause of the mismatched data types.\n",
    "\n",
    "For this output, `top` describes the most commonly occuring value in the dataset. For the fields `UTE`, `HATCH`, `WAG0N` and `SEDAN`, that value appears to be an empty string.\n",
    "\n",
    "Using the `unique()` function (see below) allows us to identify this as the root cause of our data abnormalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for cause of issues in one of the variables\n",
    "print(df['UTE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can isolate the rows containing the empty string in order to determine if there is a link between the missing variables in the data source.\n",
    "\n",
    "By using the `.values()` or `as_matrix()` functions to gather all of the readings where UTE is equal to an empty string we get the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Rows where UTE, HATCH, WAG0N or SEDAN contains empty string\n",
    "mat_view = df[(df['UTE']=='')|(df['HATCH']=='')|(df['WAG0N']=='')|(df['SEDAN']=='')].as_matrix()\n",
    "print(mat_view)\n",
    "print(\"Num Elements: \",len(mat_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "As seen above, 22 rows in the dataset contain empty string values for `UTE`, `HATCH`,`WAG0N` or `SEDAN`. This additionally identifies 22 rows where the `K_SALES_TOT` variable is also missing. A tabulated view of the results can be found below.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable Name</th>\n",
    "        <th># Missing Values</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>HATCH</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SEDAN</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>WAGON</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>UTE</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>K_SALES_TOT</td>\n",
    "        <td>22</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For the purposes of performing KMeans clustering on this data, these rows can be dropped from the dataset using the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and return dataset\n",
    "def cleanAndConvertDataset1(doc):\n",
    "    # replace empty elements with np.nan and cast fields to float\n",
    "    return df[['HATCH','SEDAN','WAG0N','UTE','K__SALES_TOT']].replace('',np.nan).astype(float)\n",
    "df = cleanAndConvertDataset1(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, visualisation of the data can assist in identifying any remaining data problems.\n",
    "Using `seaborn` and `matplotlib`; we can graph the distributions for `UTE`, `HATCH`,`WAGON` and `SEDAN` in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of UTE\n",
    "ute_dist = sns.distplot(df['UTE'].dropna())\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Hatch\n",
    "hatch_dist = sns.distplot(df['HATCH'].dropna())\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Wagon\n",
    "wagon_dist = sns.distplot(df['WAG0N'].dropna())\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Sedan\n",
    "sedan_dist = sns.distplot(df['SEDAN'].dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the distplots, there are a number of anomalies for the distribution of `WAG0N` in the dataset - particularly for wagon sales below 200 units. \n",
    "\n",
    "In order to quanitfy the anomalous values, we can increasae the number of bins and isolate the data to sales below 200 units as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Wagon\n",
    "wagon_dist = sns.distplot(df[df['WAG0N'] < 200]['WAG0N'].dropna(),bins=100)\n",
    "plt.show()\n",
    "\n",
    "# Print rows with the bottom 10 sales for the station wagons\n",
    "print(df.sort_values(by='WAG0N')[df['WAG0N'] < 200].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "As shown above, the distplot for stores selling fewer than 200 station wagons shows an unusually high number of stores selling fewer than 50 station wagons.\n",
    "\n",
    "Additionally, by comparing sales of Station Wagon against sales of other car models in these instances, We can identify a number of cases where the sales of Station Wagons are over 10 times lower than the nearest model at the same dealership. This pattern appears to be indicative of erronious data. As K-Means clustering is sensitive to outliers, these rows should be dropped from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Outliers from WAG0N\n",
    "df1 = df[df['WAG0N']>30]\n",
    "\n",
    "# Distribution of Wagon\n",
    "wagon_dist = sns.distplot(df['WAG0N'].dropna())\n",
    "wagon_dist = sns.distplot(df1['WAG0N'].dropna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "For the purposes of clustering, we will include the `HATCH`, `SEDAN`, `WAG0N` and `UTE` variables.\n",
    "These variables measure the number of sales of a particular model of vehicle made by particular a car dealership (signified by `LOCATION_NUMBER` or `DEALER_CODE`).\n",
    "\n",
    "The variables `LOCATION_NUMBER` and `DEALER_CODE` will not be included in the analysis, as unique variables do not contribute to clustering models. Additionally, `REPORT_DATE` will not be included in the analysis as it contains the same value for all observations. Finally, `K_SALES_TOT` will also be discluded from the Dataset, as the derrived value is not useful for our analysis.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "        <th>Variable</th>\n",
    "        <th>Role</th>\n",
    "        <th>Measurement Level Set</th>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>LOCATION_NUMBER</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>REPORT_DATE</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>DEALER_CODE</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>HATCH</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>WAG0N</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>SEDAN</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>UTE</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>K__SALES_TOT</td>\n",
    "        <td>Input</td>\n",
    "        <td>Interval</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[['UTE','HATCH','WAG0N','SEDAN']].dropna()\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify a car model that is underperforming in sales. Based on your reporting, the company does not want to focus thir efforts on this car model anymore and has decided to drop it from manufacturing. Now onwards, the selected car product should not be part of analysis\n",
    "By comparing the distributions of each model of car, it is possible to identify how car sales are distributed among dealerships.\n",
    "\n",
    "Below are the sales distributions of each car model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of all Models\n",
    "comb_dist = sns.distplot(df2['UTE'].dropna(),kde_kws={\"label\":\"UTE\"})\n",
    "comb_dist = sns.distplot(df2['HATCH'].dropna(),kde_kws={\"label\":\"HATCH\"})\n",
    "comb_dist = sns.distplot(df2['WAG0N'].dropna(),kde_kws={\"label\":\"WAG0N\"})\n",
    "comb_dist = sns.distplot(df2['SEDAN'].dropna(),kde_kws={\"label\":\"SEDAN\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the `UTE` model has the poorest performance among vehicle models, with the majority of dealerships selling fewer than 100 models over the sales period.\n",
    "\n",
    "Due to this, the `UTE` model will be removed from the analysis as instructed by the brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop underperforming vehicle UTE\n",
    "df2.drop(['UTE'],axis=1,inplace=True)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. The First Clustering Model\n",
    "Below is the method that will be used to perform clustering in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performKMeansClustering(docIn, k=3, scale=False,rs=42, report_cluster_distances=True):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on provided dataset and assign Model ID to each row\n",
    "    \"\"\"\n",
    "    # Create copy of input dataset to return\n",
    "    doc = docIn.copy()\n",
    "    \n",
    "    # convert the dataset into a matrix\n",
    "    X = doc.as_matrix()\n",
    "    \n",
    "    if scale:\n",
    "        # scale the variables\n",
    "        s = StandardScaler()\n",
    "        X = s.fit_transform(X)\n",
    "\n",
    "    # set the random state for the model\n",
    "    model = KMeans(n_clusters=k, random_state=rs).fit(X)\n",
    "\n",
    "    if report_cluster_distances:\n",
    "        # report sum of intra-cluster distances\n",
    "        print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "        print(\"Centroid Locations:\")\n",
    "        for c in model.cluster_centers_:print(c)\n",
    "            \n",
    "    # assign cluster to each record in X\n",
    "    y = model.predict(X)\n",
    "    \n",
    "    # Assign cluster id's to each row and return \n",
    "    doc['Cluster_ID'] = y\n",
    "    \n",
    "    return doc, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build a default clustering model with K = 3\n",
    "Using the method above, clustering is performed on the dataset using the default K of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering on dataset without normalisation\n",
    "K3_NoScale, X_NoScale = performKMeansClustering(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportClusterMembership(doc):\n",
    "    # print the number of members for each cluster\n",
    "    print(\"Distribution of Cluster Members\")\n",
    "    print(doc['Cluster_ID'].value_counts())\n",
    "    \n",
    "reportClusterMembership(K3_NoScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. How many records are assigned into each cluster?\n",
    "As seen in the output of the `reportClusterMembership()` method above, the cluster membership is as follows:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Cluster ID</th>\n",
    "<th>Number of members</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>264</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>189</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>167</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Plot the cluster distribution using pairplot. Explain key characteristics of each cluster/segment\n",
    "The key characteristics of each cluster are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visCluster(doc):\n",
    "    \"\"\"\n",
    "    Generate Pair Plot to visualise clusters\n",
    "    \"\"\"\n",
    "    # Generate Pairplot for clusters\n",
    "    cluster = sns.pairplot(doc, hue='Cluster_ID')\n",
    "    plt.show()\n",
    "\n",
    "# Plot clusters as pairplot\n",
    "visCluster(K3_NoScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterDistributions(doc, n_bins=20, cols=[], clusters=[]):\n",
    "    \"\"\"\n",
    "    Plot distribution of variables in each cluster, in relation to distributions of varibles in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # if specific clusters not provided, go over entire range\n",
    "    if len(clusters) == 0:\n",
    "        clusters_to_inspect = doc['Cluster_ID'].unique()\n",
    "\n",
    "    for cluster in clusters_to_inspect:\n",
    "        # inspecting cluster 0\n",
    "        print(\"Distribution for cluster {}\".format(cluster))\n",
    "\n",
    "        # create subplots\n",
    "        fig, ax = plt.subplots(nrows=3)\n",
    "        ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "        \n",
    "        \n",
    "\n",
    "        for j, col in enumerate(cols):\n",
    "            # create the bins\n",
    "            bins = np.linspace(min(doc[col]), max(doc[col]), 20)\n",
    "            # plot distribution of the cluster using histogram\n",
    "            sns.distplot(doc[doc['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "            # plot the normal distribution with a black line\n",
    "            sns.distplot(doc[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# prepare the column and bin size. Increase bin size to be more specific, but 20 is more than enough\n",
    "cols = ['HATCH', 'WAG0N', 'SEDAN']\n",
    "clusterDistributions(K3_NoScale,cols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 0\n",
    "As seen in the distributions for cluster 0:\n",
    "Dealerships in cluster 0 sell an average number of all vehicles. Due to the volume of sales for each vehicle, this can be interpreted as a predominance in sales of Hatch and Sedan models \n",
    "\n",
    "#### Cluster 1\n",
    "As seen in the distributions for cluster 1:\n",
    "Dealerships in cluster 1 sell the mostly Hatch Back models and average to low numbers of Station Wagons and Sedans respectively. Due to the volume of sales, this could be interpreted as selling a majority of Hatch Back models with fewer sales of Sedan models.\n",
    "\n",
    "#### Cluster 2\n",
    "As seen in the distributions for cluster 2:\n",
    "Dealerships in cluster 2 sell an above average number of station wagon and sedan model cars, with below average sales of Hatch Back model cars. \n",
    "\n",
    "These characteristics are corroborated by the distributions per cluster, found below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the effecti of using the standardization method on the model above? Does the variable normalization process enable a better clustering solution?\n",
    "The process above was repeated using the `StandardScalar` modlue from the `SKlearn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K means clustering with normalised variables\n",
    "K3_Scaled, X_Scaled = performKMeansClustering(df2,scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot clusters for the normalised dataset\n",
    "visCluster(K3_Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Distributions of clusters in the normalised dataset\n",
    "clusterDistributions(K3_Scaled,cols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the plots above, standardisation results in more cohesive clustering. As cluster centroids in K Means analysis are based on euclidean distances, Standardising the scales of values involved in the analysis allow variables with different ranges to be effectively compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpret the cluster analysis outcome. In other words, characterize the nature of each cluster by giving it a descriptive label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0 - Sedan Dominant Dealerships\n",
    "Dealerships belonging to Cluster 0 typically make above average sales on Sedan Models and average to below average sales on Station Wagon and Hatch Back Models\n",
    "\n",
    "Cluster 1 - Hatch Dominant Dealerships\n",
    "Dealershis belonging to Cluster 1 sell an above number of Hatch Back models and make average to below average sales of Station Wagon and Sedan Models Respectively\n",
    "\n",
    "Cluster 2 - Wagon & Sedan Dominant Dealerships\n",
    "Dealerships belonging to Cluster 2 make above average sales of Wagon and Sedan models, with below average sales of Hatch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Refining the clustering model\n",
    "### 1. Using elbow method and silhouette, find the optimal K. What is teh best K? Explain your reasoning. Evaluate the results\n",
    "Using the `computeKGraph` method below, the model inertia for each value of K between 2 and 8 can be graphed to determine the 'elbow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def computeKGraph(doc,X,rs=42,k_range=range(2,9,1)):\n",
    "    \"\"\"\n",
    "    Plot cluster model inertia for given dataset with different leves of K applied\n",
    "    \"\"\"\n",
    "    # Lists for clusters and costs\n",
    "    clusters = []\n",
    "    inertia_vals = []\n",
    "\n",
    "    # calculate inertia for k\n",
    "    for k in k_range:\n",
    "        # train with the current K value\n",
    "        model = KMeans(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "        model.fit(X)\n",
    "\n",
    "        # append model to lits\n",
    "        clusters.append(model)\n",
    "        inertia_vals.append(model.inertia_)\n",
    "\n",
    "    # Plot the outpt of inertai vs the value of K\n",
    "    plt.plot(range(2,9,1), inertia_vals, marker='*')\n",
    "    plt.show()\n",
    "    return clusters, inertia_vals\n",
    "    \n",
    "# Perform analysis on previously calculated dataset\n",
    "clusters, inertia = computeKGraph(K3_Scaled,X_Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the best number of clusters that can describe the dataset effectively? How was this obtained?\n",
    "Using the `identifyOptimalK` method below, we can determine the optimal value of K for a given dataset by identifying the model with the highest silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyOptimalK(evalclusters, X_in,verbose=False):\n",
    "    \"\"\"\n",
    "    Use the elbow rule to identify the optimal value of K\n",
    "    \"\"\"\n",
    "    # Initialise variables for storing optimal K value\n",
    "    best_K = 0\n",
    "    maxSilhouette = 0\n",
    "    \n",
    "    # Iterate through all elements in \n",
    "    for clust in evalclusters:\n",
    "        # Get silhouette score for cluster\n",
    "        sil = silhouette_score(X_in, clust.predict(X_in))\n",
    "        if sil > maxSilhouette:\n",
    "            maxSilhouette = sil\n",
    "            best_K = clust.n_clusters\n",
    "        \n",
    "        # Print verbose details on each cluster\n",
    "        if verbose:\n",
    "            print(clust)\n",
    "            print(\"Silhouette Score for K={}\".format(clust.n_clusters), sil)\n",
    "            print(\"-\"*20)\n",
    "    \n",
    "    # Print results and return K\n",
    "    print(\"Optimal K = \",best_K, \"with score \", maxSilhouette)\n",
    "    return best_K\n",
    "        \n",
    "# Compute optimal K based on the provided dataset\n",
    "optimal_K = identifyOptimalK(clusters,X_Scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the output above, the optimal value of K for the dataset was determined to be 4. This results in the following clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering on dataset without normalisation\n",
    "K_Optimal, X_Optimal = performKMeansClustering(df2,k=optimal_K,scale=True)\n",
    "\n",
    "# Plot clusters for the normalised dataset\n",
    "visCluster(K_Optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How can the outcome of this study be used by decision makers?\n",
    "Clusters allow decision makers to identify trends in the sales of vehicle models for each dealership.\n",
    "\n",
    "This information can be used to optimise marketing material to cater specifically to the predominent modles sold by dealerships belonging to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Association Mining and it's data Pre-processing<a name=\"association\"></a>\n",
    "\n",
    "### 1. Can you identify data quality issues in this dataset for performing association mining?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the transaction dataset\n",
    "df = pd.read_csv('Casestudy2-Data-Py/pos_transactions.csv')\n",
    "\n",
    "# info and the first 10 transactions\n",
    "print(df.info())\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first 10 transaction shown above, we can easily see that there are duplicated transactions in the dataset.\n",
    "That can be seen clearly by grouping the data (see the result below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.columns.tolist(),as_index=False).size().sort_values(ascending = False).reset_index().rename(columns={0: 'Frequency'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Frequency' column in the result above indicates the number of duplicated transactions for each unique transaction in the dataset. Therefore, we will drop those duplicated transactions except for the first occurence. Hence the number of transactions will be decreased that help the apriori algorithm (in the question 3)run faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=None, keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As were are looking to generate association rules from items purchased by each transaction, we need to group our Transaction_Id and then generate a list of all items purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by Transaction_Id, then list all items\n",
    "transactions = df.groupby(['Transaction_Id'])['Product_Name'].apply(list)\n",
    "\n",
    "print(transactions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What variables did you include in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "Association mining usually consists of two variables: a transaction ID and an item. Due to the main target which is to find out the associations between items purchased from the health and beauty aids department and the stationary department, the Product_Name should be chosen as the target variable and the Transaction_ID is chosen as ID. In summary, the variables included in the analysis as the below table.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable Name</th>\n",
    "        <th>Role</th>\n",
    "        <th>Measurement Level</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Product_Name</td>\n",
    "        <td>Target</td>\n",
    "        <td>Nominal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Transaction_Id</td>\n",
    "        <td>ID</td>\n",
    "        <td>Nominal</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conduct association mining and answer the following:\n",
    "#### a. What is the highest lift value for the resulting rules? Which rule has this value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the transactions table contains all items purchased in each transaction, we will run the apyori model with the pre-processed transactions and min_support of 0.01. This min support is chosen because the number of rules decreases when the min support increases (see the table below) \n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Min Support</th>\n",
    "        <th>Number of rules</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.01</td>\n",
    "        <td>72</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.02</td>\n",
    "        <td>38</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0.03</td>\n",
    "        <td>29</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "# type cast the transactions from pandas into normal list format and run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=0.01))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift']) \n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(len(result_df))\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out the highest lift value for the resulting rules, we will sort the rules by Lift using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort all acquired rules descending by lift# sort a \n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(len(result_df))\n",
    "print(result_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the highest lift value is 3.60\n",
    "There are two rules having this value: Perfume -> Toothbrush and Toothbrush -> Perfume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What is the highest confidence value for the resulting rules? Which rule has this value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out the highest confidence value for the resulting rules, we will sort the rules by Confidence using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort all acquired rules descending by Confidence# sort a \n",
    "result_df = result_df.sort_values(by='Confidence', ascending=False)\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, the highest confidence value is 46.37%.\n",
    "The rule has this value is Magazie & Greeding Cards -> Candy Bar  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Plot the confidence, lift, support of the resulting rules? Interpret them to discuss the rule-set obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "support = result_df['Support'].as_matrix()\n",
    "confidence = result_df['Confidence'].as_matrix()\n",
    "lift = result_df['Lift'].as_matrix()\n",
    "lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(3, figsize=(7,5))\n",
    "plt.scatter(support, confidence, c=lift, cmap='Reds')\n",
    "plt.xlabel('support')\n",
    "plt.ylabel('confidence') \n",
    "\n",
    "plt.colorbar().set_label('Lift')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows describes the support, confidence and lift for rules discovered during the association mining step.\n",
    "\n",
    "Support describes the probability of a rule appearing within the dataset. With respects to the data, Support describes the frequency with which Itemset A appears with Itemset B in the data.\n",
    "`Frequency of A and B/Total number of Rows`\n",
    "\n",
    "Confidence of rule A=>B describes the probability of A and B occuring together in all elements containing A. With respects to the data, Confidence measures how frequently item A is purchased with item B against the frequency with which only A is present.\n",
    "`Frequency of A and B/Frequency of only A`\n",
    "\n",
    "Support and Confidence are important metrics for discovering the strength of a given association rule, however, high confidence is not necessarily indicative of a useful rule. Lift describes a ratio of confidence against support; This represents the 'predictive power' of a rule compared to random chance.\n",
    "For example, a lift of 3 for A=>B indicates that customers who purchase A are 3 times more likely to also purchase B\n",
    "\n",
    "- As we can see from the figure above, the rules having lift values more than 2.0 have high confidence and low support that is a good indicator. \n",
    "- The rules having low lift values have low support that is not a good indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The store is particularly interested in products that individuals purchase when they buy “Pens”.\n",
    "#### a. How many rules are in the subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[result_df['Left_side'] == 'Pens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 3 rules are in the subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Based on the rules, what are the other products these individuals are most likely to purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pensDf = result_df[result_df['Left_side'] == 'Pens']\n",
    "otherProducts = pensDf.groupby(['Left_side'])['Right_side'].apply(list)\n",
    "otherProducts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other products these individuals are most like ly to purchase are Magazine, Candy Bar, Toothpaste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How the outcome of this study can be used by decision makers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the interest of the sore in determining the associations between items purchased from the health and beauty aids department and the stationary deparment, it should be benefit to consider the folowwing rules:\n",
    "\n",
    "Toothpaste,Pencils  =>  Candy Bar\n",
    "\n",
    "Magazine,Greeting Cards  =>  Candy Bar\n",
    "\n",
    "Toothpaste,Magazine  =>  Candy Bar\n",
    "\n",
    "Candy Bar,Magazine  =>  Greeting Cards\n",
    "\n",
    "Candy Bar,Greeting Cards  =>  Magazine\n",
    "\n",
    "Toothpaste,Greeting Cards  =>  Candy Bar\n",
    "\n",
    "Pencils,Candy Bar  =>  Magazine\n",
    "\n",
    "Pencils,Magazine  =>  Greeting Cards\n",
    "\n",
    "Candy Bar,Magazine  =>  Toothpaste\n",
    "\n",
    "Magazine,Greeting Cards  =>  Pencils\n",
    "\n",
    "Toothbrush  =>  Perfume\n",
    "\n",
    "The decision makers can use the rules extracted from this association mining to obtain the understainding of consumers' needs and behaviours. Based on these rules, several marketing strategies can be made, for example:\n",
    "\n",
    "- The association rules can tell decision makers that there are several items that are often bought together. Therefore, promoting the items together can help increase the revenue. For example, promoting the combination of Candy Bar & Greeting Cards, Toothpase & Pencils, etc. \n",
    "- These rules can also be used to structure supermarket layout. For example, putting Gretting Cards on the shelves next to the Candy Bar shelves\n",
    "- Based on the outcome of association mining, the purchasing pattern can be derrived. From there, customer segments can be identified\n",
    "- Additionally, the outcome can be used to design catologies for the supermarket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Text Mining<a name=\"text\"></a>\n",
    "\n",
    "### 1.\tWhat variables did you include in the analysis and what were their roles and measurement level set? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected column is the TEXT column, as this column contains the text data from which information will be extracted. All other columns are rejected.\n",
    "\n",
    "The LANGUANGE, OMITTED, TRUNCATED, and EXTENSION columns are unary, and provide no meaningful data. Similarly, the URI, NAME, and FILTERED columns are incremental values that also provide no meaningful data. Finally, the columns CREATED, ACCESSED, and MODIFIED, appear to be date based values. Modified is binary, and the other columns only have thirty five unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tf = pd.read_csv(\"Casestudy2-Data-Py/bbc.csv\")\n",
    "\n",
    "checkCols = [\"CREATED\", \"ACCESSED\", \"MODIFIED\"]\n",
    "\n",
    "for col in checkCols:\n",
    "    print(col, \"- Unique:\", len(tf[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Can you identify data quality issues in order to perform text mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare the text for mining, it was first stripped of punctuation and analytically useless stopwords obtained from nltk StopWords. Secondly, the text is lemmatised using the nltk WordNet dictionary, in order to reduce words to their base format. Finally, the text is vectorised using the sklearn tf idf vectoriser, so that it can be used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# initialise WordNet lemmatizer and punctuation filter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "stopwords = set(sw.words('english'))\n",
    "stopwords.update('�')\n",
    "textColumn = 'TEXT'\n",
    "clusters = 7\n",
    "rs = 42\n",
    "docTotal = len(tf[textColumn]) #store total documents for calculating document frequency percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # initialize token list\n",
    "    tokens = []\n",
    "    \n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # split the document into tokens and then create part of speech tag for each token\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the tokens list\n",
    "            lemma = lemmatize(token, tag)\n",
    "            tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "weights = tfidf_vec.idf_\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "print(\"Unique tokens:\", len(tfidf_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without document and term frequency filtering, there are 36340 unique tokens found by the tf idf vectorizer. This is far too many to generate meaningful clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# K means clustering using the term vector\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualise text cluster. Useful for the assignment too :)\n",
    "def visualise_text_cluster(n_clusters, cluster_centers, terms, num_word = 7):\n",
    "    # -- Params --\n",
    "    # cluster_centers: cluster centers of fitted/trained KMeans/other centroid-based clustering\n",
    "    # terms: terms used for clustering\n",
    "    # num_word: number of terms to show per cluster. Change as you please.\n",
    "    \n",
    "    # find features/terms closest to centroids\n",
    "    ordered_centroids = cluster_centers.argsort()[:, ::-1]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        print(\"Top terms for cluster {}:\".format(cluster), end=\" \")\n",
    "        for term_idx in ordered_centroids[cluster, :num_word]:\n",
    "            print(terms[term_idx], end=', ')\n",
    "        print()\n",
    "        \n",
    "# call it\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the clusters generated with unfiltered terms are damaged by high frequency, low value terms, such as terms like 'say', 'play', and 'ball'. These terms are vague and do not allow for meaningful conclusions to be drawn from the clusters. As such, the terms must be filtered using term/document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Based on the ZIPF plot, list the top 10 terms that will be least useful for clustering purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# creating tf-idf terms - a bit slow, do it occasionaly\n",
    "def calculate_tf_idf_terms(document_col):\n",
    "    # Param - document_col: collection of raw document text that you want to analyse\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "    X_count = count_vec.fit_transform(tf[textColumn])\n",
    "    \n",
    "    # create list of terms and their tf and df\n",
    "    terms = [{'term': t, 'idx': count_vec.vocabulary_[t],\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero(),\n",
    "              'weights': weights[i]}\n",
    "             for i, t in enumerate(count_vec.vocabulary_)]\n",
    "    \n",
    "    return terms\n",
    "    \n",
    "terms = calculate_tf_idf_terms(tf[textColumn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation of ZIPF law\n",
    "def visualise_zipf(terms, itr_step = 50):\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    from math import sqrt\n",
    "    \n",
    "    # --- Param ---\n",
    "    # terms: collection of terms dictionary from calculate_tf_idf_terms function\n",
    "    # itr_step: used to control how many terms that you want to plot. Num of terms to plot = N terms / itr_step\n",
    "    \n",
    "    # sort terms by its frequency\n",
    "    terms.sort(key=lambda x: (x['tf'], x['df']), reverse=True)\n",
    "    \n",
    "    # select a few of the terms for plotting purpose\n",
    "    sel_terms = [terms[i] for i in range(0, len(terms), itr_step)]\n",
    "    labels = [term['term'] for term in sel_terms]\n",
    "    \n",
    "    # plot term frequency ranking vs its DF\n",
    "    plt.plot(range(len(sel_terms)), [x['df'] for x in sel_terms])\n",
    "    plt.xlabel('Term frequency ranking')\n",
    "    plt.ylabel('Document frequency')\n",
    "    \n",
    "    max_x = len(sel_terms)\n",
    "    max_y = max([x['df'] for x in sel_terms])\n",
    "    \n",
    "    # annotate the points\n",
    "    prev_x, prev_y = 0, 0\n",
    "    for label, x, y in zip(labels,range(len(sel_terms)), [x['df'] for x in sel_terms]):\n",
    "        # calculate the relative distance between labels to increase visibility\n",
    "        x_dist = (abs(x - prev_x) / float(max_x)) ** 2\n",
    "        y_dist = (abs(y - prev_y) / float(max_y)) ** 2\n",
    "        scaled_dist = sqrt(x_dist + y_dist)\n",
    "        \n",
    "        if (scaled_dist > 0.1):\n",
    "            plt.text(x+2, y+2, label, {'ha': 'left', 'va': 'bottom'}, rotation=30)\n",
    "            prev_x, prev_y = x, y\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return terms\n",
    "\n",
    "#Store terms ordered by term frequency, document frequency\n",
    "orderedTerms = visualise_zipf(terms).copy()\n",
    "\n",
    "#Reorder terms by weights\n",
    "terms.sort(key=lambda x: (x['weights']), reverse=True)\n",
    "for i, term in enumerate(terms[0:10]):\n",
    "    print(str(i + 1) + \")\\tTerm:\", term['term'], \"\\tFrequency:\", term['tf'], \"\\tWeight:\", term['weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ZIPF law, the least useful terms can be calculated. As visualised above, the Top 10 least useful terms by weight are:\n",
    "\n",
    "1. say\n",
    "2. win\n",
    "3. one\n",
    "4. go\n",
    "5. two\n",
    "6. time\n",
    "7. first\n",
    "8. get\n",
    "9. player\n",
    "10. make\n",
    "\n",
    "These terms have the highest tf-idf weight values, and thus provide no meaningful value to the clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Did you disregard any frequent terms? Justify their selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the top 25 most frequent terms from the dataset, it can be observed that there is a large amount of vague sports terminology. All of the top ten least useful terms can be observed in the top 25 most frequent. The most frequent term, 'say', also provides no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An array of zeroes equal to the number of documents, to count document frequencies\n",
    "dfCount = [0] * docTotal\n",
    "\n",
    "for i, term in enumerate(orderedTerms):\n",
    "    if i < 25:\n",
    "        print(str(i + 1) + \")\\tTerm:\", term['term'], \"\\tDocument Frequency:\", (term['df'] / docTotal * 100), \"%\\tTerm Frequency\", term['tf'])\n",
    "    dfCount[term['df'] - 1] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the high frequency terms occur within 40-50% of the documents, except for the 9th term, 'england'. This term can be useful for clustering, and may provide useful information. All terms above 40% document frequency (appearing in over 80 documents), will be disregarded through filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=1, max_df=0.4)\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "tokenCount = len(tfidf_vec.get_feature_names())\n",
    "print(\"Unique tokens for 40%:\", tokenCount)\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the maximum document frequency to 40% removes 20 terms, which is terms 1-21, excluding 'england'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Justify the term weighting option selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum document frequency is set to 40% to remove vague, high frequency terms, as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Show distribution of document frequencies.\n",
    "plt.bar(range(1, len(dfCount) + 1), dfCount)\n",
    "plt.show()\n",
    "\n",
    "print(\"Document Frequency 1 - 5:\", sum(dfCount[0:5]), \"Remainder:\", tokenCount - sum(dfCount[0:5]),  \"Elements:\", dfCount[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35101 tokens that only appear in 5 or less documents, and over 29435 only appear in 1. Due to this large portion, filtering a higher minimum document frequency may remove too many terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=2, max_df=0.4)\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "tokenCount = len(tfidf_vec.get_feature_names())\n",
    "print(\"Unique tokens for min_df = 2:\", tokenCount)\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())\n",
    "\n",
    "print()\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.4)\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "tokenCount = len(tfidf_vec.get_feature_names())\n",
    "print(\"Unique tokens for min_df = 3:\", tokenCount)\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())\n",
    "\n",
    "print()\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=4, max_df=0.4)\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "tokenCount = len(tfidf_vec.get_feature_names())\n",
    "print(\"Unique tokens for min_df = 4:\", tokenCount)\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())\n",
    "\n",
    "print()\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(strip_accents = 'unicode', tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=5, max_df=0.4)\n",
    "X = tfidf_vec.fit_transform(tf[textColumn])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "tokenCount = len(tfidf_vec.get_feature_names())\n",
    "print(\"Unique tokens for min_df = 5:\", tokenCount)\n",
    "\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=rs).fit(X)\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the clusters generated at different min_df levels, setting the minimum document frequency to 4 produces the best results. Meaningful clusters are generated such as the cricket tests for cluster 0, and athletics bans for cluster 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is the number of input features available to execute clustering?  (For information: Note how the original text data is now converted into a feature set that can be mined for knowledge discovery?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the tfidf frequency matrix, Singular Value Decomposition was used. By setting the number of components to 100, a significantly smaller matrix is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=rs)\n",
    "X_trans = svd.fit_transform(X)\n",
    "\n",
    "# sort the components by largest weighted word\n",
    "sorted_comp = svd.components_.argsort()[:, ::-1]\n",
    "terms = tfidf_vec.get_feature_names()\n",
    "\n",
    "# visualise word - concept/component relationships\n",
    "for comp_num in range(10):\n",
    "    print(\"Top terms in component #{}\".format(comp_num), end=\" \")\n",
    "    for i in sorted_comp[comp_num, :5]:\n",
    "        print(terms[i], end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# K-means clustering using LSA-transformed X\n",
    "svd_kmeans = KMeans(n_clusters=7, random_state=rs).fit(X_trans)\n",
    "\n",
    "# transform cluster centers back to original feature space for visualisation\n",
    "original_space_centroids = svd.inverse_transform(svd_kmeans.cluster_centers_)\n",
    "\n",
    "# visualisation\n",
    "visualise_text_cluster(svd_kmeans.n_clusters, original_space_centroids, tfidf_vec.get_feature_names(), num_word = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 7. State how many clusters are generated? Name each cluster meaningfully according to the terms that appear in the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were seven clusters generated through singular value decomposition. These clusters each had meaningful categories and themes.\n",
    "\n",
    "Cluster 1: Australian Open Tennis\n",
    "\n",
    "Containing terms such as Hewitt, Federer, Australian Open and Grand Slam, this cluster is about professional tennis.\n",
    "\n",
    "Cluster 2: Six Nation Rugby Union\n",
    "      \n",
    "Featuring the term 'Six Nation' and the six nations that competed in the tournament, 'Ireland', 'England', 'Wales', 'Scotland', 'Italy' and 'France', this cluster focuses on the annual Six Nationa Rugby Union competition     \n",
    "        \n",
    "Cluster 3: Rugby Team Changes\n",
    "\n",
    "This cluster is about changes in staff and players in rugby teams. This is evidenced by the terms 'rugby', 'club', 'director', 'coach', 'new' and 'want'\n",
    "\n",
    "Cluster 4: Athletics Drug Bans\n",
    "\n",
    "With the terms 'Athens', 'Olympic', 'athletics', 'drug' and 'ban', this cluster is about drug related bans occurring at the Athens Olympics.\n",
    "\n",
    "Cluster 5: Cricket Tests\n",
    "\n",
    "Featuring cricketing nations such as 'Pakistan', 'Australia', 'South Africa' and 'India', as well as the terms 'cricket', 'test', 'wicket' and 'run'. This section is about cricket test matches between nations.\n",
    "\n",
    "Cluster 6: European Football\n",
    "\n",
    "Naming several prominent European Football clubs, 'Chelsea', 'Arsenal', 'Liverpool' and 'Manchester', this section is about the European Football League\n",
    "\n",
    "Cluster 7: Davis Cup Tennis\n",
    "\n",
    "The final cluster is about the Davis Cup in tennis. This can be shown by the terms 'Davis Cup' and the names of famous professional tennis players such as 'Roddick', 'Nadal' and 'Moya'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Identify the first fifteen high frequent terms (that are not stop words or noise) in the start list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the first fifteen most frequent terms, the remaining terms used in the singular value decomposition are extracted from the complete list of terms ordered by term frequency, created in question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdTerms = []\n",
    "for term in orderedTerms:\n",
    "    if term['term'] in tfidf_vec.get_feature_names():\n",
    "        svdTerms.append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, term in enumerate(svdTerms[0:40]):\n",
    "    print(str(i + 1) + \")\\tTerm:\", term['term'], \"\\tFrequency:\", term['tf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top fifteen terms are:\n",
    "\n",
    "1. England\n",
    "2. Team\n",
    "3. Match\n",
    "4. Second\n",
    "5. Set\n",
    "6. Give\n",
    "7. Day\n",
    "8. Open\n",
    "9. Final\n",
    "10. New\n",
    "11. Want\n",
    "12. Cup\n",
    "13. Good\n",
    "14. Think\n",
    "15. Club\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Describe how these clusters can be useful in the online personalised news story service planned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the clusters discerned in this section, there are several observations that can be made. However, the news articles focus primarily on sport. This may be a result of a biased sample, choosing only sports related articles, but this does not mean the observations are invalid. Instead, these observations can be applied specifically to sports journalism.\n",
    "\n",
    "Firstly, the clusters seem to be evenly split between different sports. Cricket, tennis, rugby union, football (soccer), and athletics are represented. This implies an even coverage of popular sports is necessary. There may be available audiences for each discipline, and focusing too hard on a singular sport will shrink the audience.\n",
    "\n",
    "Secondly, the clusters focus on the 'highest' levels of the sport. The Australian Open and Davis Cup for tennis, the international tests for cricket, and the European football leagues. The implication is that there is the most interest around the highest levels and greatest competitions of the sport.\n",
    "\n",
    "Finally, there are also clusters relating to background machinations and controversy. For example, the drug bans in the Athens Olympics and the potential new managers in European Football. This ties in to the second point, as audiences are most likely interested in the events that can affect the outcomes of their sports championships and tournaments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Web Mining<a name=\"web\"></a>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Casestudy2-Data-Py/web_log_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the provided data was removed unproductive items such as graphics, sound, etc, we can see that Javascript (.js), Cascading Style Sheets(.css) and Icon (.ico) were not completely filterred. In the context of Web usage mining, those files could be considered as non-meaningful requests. Hene, we will remove all these files as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What is the Rational behind the selected method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Web usage mining, sequential rule mining helps to discover inter-session patterns such as the presense set of pages followed by another set of pages ordered by the step visitors browsing pages. By using this approach, web owners can take advantages of the rules to predict visting patterns, then make websites layout beter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What variables were included in the analysis and what were their roles and measurement level set? Justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable Name</th>\n",
    "        <th>Role</th>\n",
    "        <th>Measurement Level</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>request</td>\n",
    "        <td>Target</td>\n",
    "        <td>Norminal</td>\n",
    "        <td>Cleaned request made to a site</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>step</td>\n",
    "        <td>Sequence</td>\n",
    "        <td>Ordinal</td>\n",
    "        <td>Ordered request access made by user</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>user_id</td>\n",
    "        <td>ID</td>\n",
    "        <td>Norminal</td>\n",
    "        <td>Identifier of user</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Can you identify data quality issues in order to perform web mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data issues have been discussed in the data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ending_slash(doc):\n",
    "    \"\"\"\n",
    "    Fix data inconsistancies by removing trailing slashes from requests\n",
    "    \"\"\"\n",
    "    # Get Request\n",
    "    request_string = doc['request']\n",
    "    # Strip slash from request\n",
    "    request_string = request_string.strip('/')\n",
    "    # Add slash back to start of request\n",
    "    request_string = '/'+request_string\n",
    "    #return cleaned request to list as new attribute\n",
    "    doc['request'] = request_string\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(strip_ending_slash, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_request_extension(doc):\n",
    "    request_string = doc['request']\n",
    "    path, extension = os.path.splitext(request_string)\n",
    "    doc['extension'] = extension\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(get_request_extension, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unproductiveItems = ['.ico', '.js', '.css']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.extension.isin(unproductiveItems)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Discuss the results obtained. Discuss also the applicability of findings of the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dafafame will be sorted by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['request', 'user_id', 'step']]\n",
    "df = df.sort_values(by=['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dafafame will be groupued by user to get a list of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['user_id'])['request'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed all the lists that have only one item or one request as it would not help in finding visitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df.values.tolist()\n",
    "\n",
    "sequences = [a for a in sequences if len(a) != 1]\n",
    "# show the first 5 sequences\n",
    "print(sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "''' Uses SPMF to find association rules in supplied transactions '''\n",
    "def get_association_rules(sequences, min_sup, min_conf):\n",
    "    # step 1: create required input for SPMF\n",
    "    \n",
    "    # prepare a dict to uniquely assign each item in the transactions to an int ID\n",
    "    item_dict = defaultdict(int)\n",
    "    output_dict = defaultdict(str)\n",
    "    item_id = 1\n",
    "    \n",
    "    # write your sequences in SPMF format\n",
    "    with open('seq_rule_input.txt', 'w+') as f:\n",
    "        for sequence in sequences:\n",
    "            z = []\n",
    "            for itemset in sequence:\n",
    "                # if there are multiple items in one itemset\n",
    "                if isinstance(itemset, list):\n",
    "                    for item in itemset:\n",
    "                        if item not in item_dict:\n",
    "                            item_dict[item] = item_id\n",
    "                            item_id += 1\n",
    "\n",
    "                        z.append(item_dict[item])\n",
    "                else:\n",
    "                    if itemset not in item_dict:\n",
    "                        item_dict[itemset] = item_id\n",
    "                        output_dict[str(item_id)] = itemset\n",
    "                        item_id += 1\n",
    "                    z.append(item_dict[itemset])\n",
    "                    \n",
    "                # end of itemset\n",
    "                z.append(-1)\n",
    "            \n",
    "            # end of a sequence\n",
    "            z.append(-2)\n",
    "            f.write(' '.join([str(x) for x in z]))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # run SPMF with supplied parameters\n",
    "    supp_param = '{}%'.format(int(min_sup * 100))\n",
    "    conf_param = '{}%'.format(int(min_conf * 100))\n",
    "    subprocess.call(['java', '-jar', 'spmf.jar', 'run', 'RuleGrowth', 'seq_rule_input.txt', 'seq_rule_output.txt', '10%', '10%'], shell=True)\n",
    "    \n",
    "    # read back the output rules\n",
    "    outputs = open('seq_rule_output.txt', 'r').read().strip().split('\\n')\n",
    "    output_rules = []\n",
    "    for rule in outputs:\n",
    "        left, right, sup, conf = re.search(pattern=r'([0-9\\,]+) ==> ([0-9\\,]+) #SUP: ([0-9]+) #CONF: ([0-9\\.]+)', string=rule).groups()\n",
    "        sup = int(sup) / len(sequences)\n",
    "        conf = float(conf)\n",
    "        output_rules.append([[output_dict[x] for x in left.split(',')], [output_dict[x] for x in right.split(',')], sup, conf])\n",
    "    \n",
    "    # return pandas DataFrame\n",
    "    return pd.DataFrame(output_rules, columns = ['Left_rule', 'Right_rule', 'Support', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_association_rules(sequences, 0.1, 0.1).sort_values(by='Confidence', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first rule is '/' => '/services.html' for 0.18 support and 0.85 confidence. This is a strong rule, as it implies 18% of visitor visited 'eaglefarm/pdf/Web_Price_List.pdf'  after '/' (support) and if visitor visited '/', the probability of them visiting 'eaglefarm/pdf/Web_Price_List.pdf' after is 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Graph Analysis\n",
    "#### a. What is the Rational behind the selected method\n",
    "By performing graph analysis on the web logs, it is possible to visualise the structure and useage of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>date_time</th>\n",
       "      <th>request</th>\n",
       "      <th>step</th>\n",
       "      <th>session</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c210-49-32-6.rochd2.</td>\n",
       "      <td>18/Apr/2005:21:25:07</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>visp.inabox.telstra.</td>\n",
       "      <td>19/Apr/2005:08:24:28</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsl-61-95-54-84.requ</td>\n",
       "      <td>19/Apr/2005:08:33:01</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d220-236-91-52.dsl.n</td>\n",
       "      <td>19/Apr/2005:09:16:06</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allptrs.eq.edu.au</td>\n",
       "      <td>19/Apr/2005:09:47:54</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpe-144-136-135-38.q</td>\n",
       "      <td>19/Apr/2005:10:13:37</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>225-145-222-203.rev.</td>\n",
       "      <td>19/Apr/2005:11:48:32</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cpe-138-130-198-54.q</td>\n",
       "      <td>19/Apr/2005:12:31:54</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>203-219-44-170-qld.t</td>\n",
       "      <td>19/Apr/2005:12:33:49</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cpe-138-130-198-54.q</td>\n",
       "      <td>19/Apr/2005:12:42:51</td>\n",
       "      <td>/</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ip             date_time request  step  session  user_id\n",
       "0  c210-49-32-6.rochd2.  18/Apr/2005:21:25:07       /     1        3        3\n",
       "1  visp.inabox.telstra.  19/Apr/2005:08:24:28       /     1       12       12\n",
       "2  dsl-61-95-54-84.requ  19/Apr/2005:08:33:01       /     1       13       13\n",
       "3  d220-236-91-52.dsl.n  19/Apr/2005:09:16:06       /     1       15       15\n",
       "4     allptrs.eq.edu.au  19/Apr/2005:09:47:54       /     1       22       22\n",
       "5  cpe-144-136-135-38.q  19/Apr/2005:10:13:37       /     1       23       23\n",
       "6  225-145-222-203.rev.  19/Apr/2005:11:48:32       /     1       25       25\n",
       "7  cpe-138-130-198-54.q  19/Apr/2005:12:31:54       /     1       26       26\n",
       "8  203-219-44-170-qld.t  19/Apr/2005:12:33:49       /     1       29       29\n",
       "9  cpe-138-130-198-54.q  19/Apr/2005:12:42:51       /     1       30       30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries required for this section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Load Data for Web log minig\n",
    "df = pd.read_csv('Casestudy2-Data-Py/web_log_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. What variables were included in the analysis and what were their roles and measurement level set? Justify your choice.\n",
    "<table>\n",
    "<tr>\n",
    "<th>Variable</th>\n",
    "<th>Measurement Level</th>\n",
    "<th>Role</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Request</td>\n",
    "<td>Nominal</td>\n",
    "<td>Node</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Step</td>\n",
    "<td>Ordinal</td>\n",
    "<td>Used to identify the edge</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>User_ID</td>\n",
    "<td>Nominal</td>\n",
    "<td>Used to group steps and requrests</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Can you identify data quality issues in order to perform web mining?\n",
    "As mentioned in Sequential Rule Mining Section, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_ending_slash(doc):\n",
    "    \"\"\"\n",
    "    Fix data inconsistancies by removing trailing slashes from requests\n",
    "    \"\"\"\n",
    "    # Get Request\n",
    "    request_string = doc['request']\n",
    "    # Strip slash from request\n",
    "    request_string = request_string.strip('/')\n",
    "    # Add slash back to start of request\n",
    "    request_string = '/'+request_string\n",
    "    #return cleaned request to list as new attribute\n",
    "    doc['request'] = request_string\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "df1 = df1.apply(strip_ending_slash, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5866 entries, 0 to 5865\n",
      "Data columns (total 6 columns):\n",
      "ip           5866 non-null object\n",
      "date_time    5866 non-null object\n",
      "request      5866 non-null object\n",
      "step         5866 non-null int64\n",
      "session      5866 non-null int64\n",
      "user_id      5866 non-null int64\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 275.0+ KB\n",
      "None\n",
      "Length Before:  114\n",
      "Length After:  96\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())\n",
    "print(\"Length Before: \",len(df.request.unique()))\n",
    "print(\"Length After: \",len(df1.request.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower(doc):\n",
    "    \"\"\"\n",
    "    Force requests to lower case to reduce unique values\n",
    "    \"\"\"\n",
    "    # Get Request from data\n",
    "    reqStr = doc['request']\n",
    "    # Cast string to lower case\n",
    "    reqStr = reqStr.lower()\n",
    "    # Add back to Dataframe as new attribute\n",
    "    doc['request'] = reqStr\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "df2 = df1.apply(make_lower,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before: \",len(df1.request.unique()))\n",
    "print(\"after: \",len(df2.request.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "web_log = df2.copy()\n",
    "web_log = web_log[['ip','request','step','user_id']]\n",
    "web_log.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(web_log.groupby(['user_id','request','ip'])['step'].value_counts())\n",
    "\n",
    "# print(web_log.groupby(['user_id'])['request'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Discuss the results obtained. Discuss also the applicability of findings of the method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildGraph(doc):\n",
    "    \"\"\"\n",
    "    Build a graphs based on requests relating to an ip\n",
    "    \n",
    "    for each unique ip create a node\n",
    "    for each request for that ip create a node and edge\n",
    "    \"\"\"\n",
    "    print(\"generating graph...\")\n",
    "    \n",
    "    # Begin Graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Group Elements by user and get a list of unique users to iterate through\n",
    "    sessions = doc.groupby(['user_id'])\n",
    "    users = doc['user_id'].unique()\n",
    "    \n",
    "    for user in users:\n",
    "        # Get subset of dataframe for each user ordered by step\n",
    "        \n",
    "        # Initialise a variable to hold the previous request\n",
    "        prev_req = ''\n",
    "        \n",
    "        session_requests = sessions.get_group(user).sort_values(by=['step'])\n",
    "        for index, row in session_requests.iterrows():\n",
    "            #for each step in the session\n",
    "            req = row['request']\n",
    "            try:\n",
    "                # Add Current request to structure\n",
    "                G.add_node(req)\n",
    "                \n",
    "                if prev_req != '':\n",
    "                    G.add_edge(req,prev_req)\n",
    "                prev_req = req\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    Nn = G.number_of_nodes()\n",
    "    Ne = G.number_of_edges()\n",
    "    print('Complete. there are {} nodes and {} edges in the graph'.format(Nn,Ne))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(G,nodeList=None, labels=False, layout='spring',nAlpha=0.2,nCol='blue',nSize=600,eAlpha=1, eCol='blue',lSize=8):\n",
    "    if layout == 'spring':\n",
    "        pos = nx.spring_layout(G)\n",
    "    elif layout == 'shell':\n",
    "        pos = nx.shell_layout(G)\n",
    "    else:\n",
    "        pos = nx.spectral_layout(G)\n",
    "        \n",
    "    # Draw Nodes\n",
    "    deg = nx.degree(G)\n",
    "    \n",
    "    plt.figure(3,figsize=(20,20))\n",
    "    \n",
    "    if nodeList == None:\n",
    "        nx.draw_networkx_nodes(G,pos, alpha=nAlpha,color=nCol,node_size=[d[1]*nSize for d in deg])#[d[1]**2 for d in deg]\n",
    "    else:\n",
    "        nx.draw_networkx_nodes(G,pos,nodelist=nodeLists, alpha=nAlpha,color=nCol,node_size=[d[1]*nSize for d in deg])#[d[1]**2 for d in deg]\n",
    "\n",
    "    if labels:\n",
    "        # Draw Labels\n",
    "        nx.draw_networkx_labels(G,pos,font_size=lSize)\n",
    "#         nx.draw_networkx_labels(G,pos)\n",
    "    \n",
    "    # Draw Edges\n",
    "    nx.draw_networkx_edges(G,pos,alpha=eAlpha,edge_color=eCol)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask= web_log['request'].value_counts()>30\n",
    "def shouldTrim(doc):\n",
    "    drequest = doc['request']\n",
    "    doc['drop'] = mask[drequest]\n",
    "    return doc\n",
    "web_trim = web_log.apply(shouldTrim,axis=1)\n",
    "web_trim = web_trim[web_trim['drop']==True]\n",
    "\n",
    "# G2 = buildGraph(web_trimmed)\n",
    "# drawGraph(G2,nSize=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = buildGraph(web_trim)\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True,layout='spring')\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True,layout='shell')\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G = buildGraph(web_trim)\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True,layout='spring')\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True,layout='shell')\n",
    "drawGraph(G,nSize=2,eAlpha=0.2,labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for centrality analysis\n",
    "import community\n",
    "\n",
    "def centrality(G, Nimportant=5, method='degree'):\n",
    "    \"\"\"\n",
    "        returns a subgraph of G with the most important nodes according to the centrality algorithm chosen\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'eigenvector':\n",
    "        ranking = nx.eigenvector_centrality_numpy(G)\n",
    "    elif method == 'degree':\n",
    "        dn = nx.degree(G)\n",
    "        degs = [dn[u] for u, i in dn]; degs.sort(); degs = degs[::-1]; degs = degs[:Nimportant]\n",
    "        important_nodes = [u for u, i in dn if dn[u] in degs]\n",
    "        Gt=G.subgraph(important_nodes)\n",
    "        important_nodes = [(u,dn[u]) for u,i in dn if dn[u] in degs]\n",
    "    else:\n",
    "        phi = (1+math.sqrt(5))/2.0 # largest eigenvalue of adj matrix\n",
    "        ranking = nx.katz_centrality_numpy(G,1/phi)\n",
    "    if method != 'degree':\n",
    "        important_nodes = sorted(ranking.items(), key=operator.itemgetter(1))[::-1][0:Nimportant]\n",
    "        dnodes=[n[0] for n in important_nodes]\n",
    "        Gt = G.subgraph(dnodes)\n",
    "    with open('Centrality_details.txt','w') as f:\n",
    "        for var in important_nodes:\n",
    "            f.write('%s\\t %.3f\\n' %(var[0],var[1]))\n",
    "    \n",
    "    return Gt\n",
    "\n",
    "Gt = centrality(G, Nimportant=10, method='degree')\n",
    "drawGraph(Gt, labels=True,lSize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import warnings, numpy as np\n",
    "\n",
    "''' Find communities in a social media graph G '''\n",
    "def communities(G):\n",
    "    # close all plots where possible\n",
    "    try:\n",
    "        plt.clf(); plt.cla(); plt.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # find best partition\n",
    "    part = community.best_partition(G)\n",
    "    values = [part.get(node) for node in G.nodes()]  # get labels\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        nx.draw_spring(G, cmap = plt.get_cmap('jet'), node_color = values, node_size=30, with_labels=False)\n",
    "        plt.savefig('Communities.png')\n",
    "        plt.show()\n",
    "    \n",
    "    # measure number of communities and network modularity\n",
    "    mod, k = community.modularity(part,G), len(set(part.values()))\n",
    "    print(\"Number of Communities = %d\\nNetwork modularity = %.2f\" %(k,mod)) # https://en.wikipedia.org/wiki/Modularity_%28networks%29\n",
    "    \n",
    "    # get members of each partition\n",
    "    part_members, part_len, dCommunity = {}, {}, []\n",
    "    for usr,par in part.items():\n",
    "        if par not in part_members.keys():\n",
    "            part_members[par]=[usr]; part_len[par]=1\n",
    "        else:\n",
    "            part_members[par]=part_members[par]+[usr]; part_len[par]+=1\n",
    "    \n",
    "    # save community details into text file\n",
    "    with open('Community_details.txt','w') as f:\n",
    "        for com in part_len:\n",
    "            dCommunity.append((com, part_len[com], set(part_members[com])))\n",
    "            f.write('community %d\\t N=%d\\t users= ' %(com,part_len[com]))\n",
    "            f.write('%s\\n' %(' '.join(part_members[com])))\n",
    "    \n",
    "    dCommunity.sort(key=lambda tup: tup[1])\n",
    "    return dCommunity[::-1]\n",
    "\n",
    "com = communities(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
